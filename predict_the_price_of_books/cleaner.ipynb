{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b0dfc0-bb51-420a-b525-775a3575a9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/anaconda3/envs/kaggle/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer,\n",
    "    HashingVectorizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9f0977-8fd8-44c7-8602-fd26fbb29db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to generate word embbeding vectors\n",
    "glove_input_file = \"/Users/kdang/Documents/glove.6B/glove.6B.100d.txt\"\n",
    "word2vec_output_file = \"glove.6B.100d.txt.word2vec\"\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "word_model = KeyedVectors.load_word2vec_format(word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7b97d6-284c-4e2d-bc7d-86051032f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel(\"Participants_Data/Data_Train.xlsx\")\n",
    "test = pd.read_excel(\"Participants_Data/Data_Test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563fc1a3-96ac-4e04-b022-9f78a31e442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"type\"] = \"train\"\n",
    "test[\"type\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa4f3e7a-185c-4c6a-8008-b46847983b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9ff91d-7b70-4722-acde-cdb46f7c53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7797 entries, 0 to 7796\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   index         7797 non-null   int64  \n",
      " 1   Title         7797 non-null   object \n",
      " 2   Author        7797 non-null   object \n",
      " 3   Edition       7797 non-null   object \n",
      " 4   Reviews       7797 non-null   object \n",
      " 5   Ratings       7797 non-null   object \n",
      " 6   Synopsis      7797 non-null   object \n",
      " 7   Genre         7797 non-null   object \n",
      " 8   BookCategory  7797 non-null   object \n",
      " 9   Price         6237 non-null   float64\n",
      " 10  type          7797 non-null   object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 670.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee718de2-092a-4bc8-a59e-8c171c3ac435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/anaconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/strings/accessor.py:101: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df[\"RatingValue\"] = df[\"Reviews\"].apply(lambda x: float(x.split()[0]) / 5)\n",
    "df[\"NumReview\"] = df[\"Ratings\"].apply(lambda x: int(x.split()[0].replace(\",\", \"\")))\n",
    "months = [\n",
    "    \"Jan\",\n",
    "    \"Feb\",\n",
    "    \"Mar\",\n",
    "    \"Apr\",\n",
    "    \"May\",\n",
    "    \"Jun\",\n",
    "    \"Jul\",\n",
    "    \"Aug\",\n",
    "    \"Sep\",\n",
    "    \"Oct\",\n",
    "    \"Nov\",\n",
    "    \"Dec\",\n",
    "]\n",
    "monthsMaxDays = {\n",
    "    \"Jan\": 31,\n",
    "    \"Feb\": 28,\n",
    "    \"Mar\": 31,\n",
    "    \"Apr\": 30,\n",
    "    \"May\": 31,\n",
    "    \"Jun\": 30,\n",
    "    \"Jul\": 31,\n",
    "    \"Aug\": 31,\n",
    "    \"Sep\": 30,\n",
    "    \"Oct\": 31,\n",
    "    \"Nov\": 30,\n",
    "    \"Dec\": 31,\n",
    "}\n",
    "months_to_num = {}\n",
    "for i, m in enumerate(months):\n",
    "    months_to_num[m] = i + 1\n",
    "df[\"Edition\"] = df.Edition.apply(lambda x: x.replace(\"â€“\", \"-\"))\n",
    "df.loc[df.Edition.str.match(\".*\\(\\w+\\).*\"), \"Edition\"] = df.loc[\n",
    "    df.Edition.str.match(\".*\\(\\w+\\).*\"), \"Edition\"\n",
    "].apply(lambda x: re.sub(\"\\(\\w+\\),\", \"\", x))\n",
    "df.loc[\n",
    "    df.Edition.str.match(r\".*[^\\d]+$\"), \"EditionDate\"\n",
    "] = np.nan  # pattern 1, year and month\n",
    "df.loc[df.Edition.str.match(r\".*\\d+$\"), \"EditionDate\"] = df.Edition.apply(\n",
    "    lambda x: x.split(\",\")[-1].split(\"-\")[-1].strip()\n",
    ")\n",
    "df[\"Year\"] = df.EditionDate.apply(\n",
    "    lambda x: int(x[-4:].strip()) if pd.notnull(x) else np.nan\n",
    ")\n",
    "df[\"Month\"] = df.EditionDate.apply(\n",
    "    lambda x: x[-8:-4] if (pd.notnull(x) and len(x) > 8) else np.nan\n",
    ")\n",
    "df[\"Day\"] = df.EditionDate.apply(\n",
    "    lambda x: int(x[-11:-9].strip()) if (pd.notnull(x) and len(x) >= 10) else np.nan\n",
    ")\n",
    "df[\"PrintEdition\"] = df.Edition.apply(lambda x: x.split(\",\")[0])\n",
    "df[\"PrintEdition\"] = df[\"PrintEdition\"].map(df[\"PrintEdition\"].value_counts())\n",
    "df[\"IsImported\"] = df.Edition.str.contains(\"Import\")\n",
    "df[\"IsBook\"] = df.Genre.str.contains(\"(Books)\")\n",
    "df[\"IsMultipleAuthor\"] = df.Author.str.match(\".*[,&-].*\")\n",
    "df[\"IsSpecialAuthor\"] = df.Author.apply(\n",
    "    lambda x: any(\n",
    "        i\n",
    "        for i in x.lower().split()\n",
    "        if i\n",
    "        in [\n",
    "            \"phd.\",\n",
    "            \"phd\",\n",
    "            \"dr\",\n",
    "            \"dr.\",\n",
    "            \"prof\",\n",
    "            \"prof.\",\n",
    "            \"sir\",\n",
    "            \"sir.\",\n",
    "            \"m.d.\",\n",
    "            \"m.d.\",\n",
    "            \"mr\",\n",
    "            \"mr.\",\n",
    "            \"mrs\",\n",
    "            \"mrs.\",\n",
    "            \"m.a\",\n",
    "            \"m.a.\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "df[\"IsFamousAuthor\"] = df.NumReview > 100\n",
    "df[\"BookCategory\"] = df[\"BookCategory\"].map(df[\"BookCategory\"].value_counts())\n",
    "df[\"Genre\"] = df[\"Genre\"].map(df[\"Genre\"].value_counts())\n",
    "df.loc[pd.isnull(df.Year), \"Year\"] = df.loc[pd.isnull(df.Year), \"Year\"].apply(\n",
    "    lambda x: random.randrange(df.Year.min(), df.Year.max())\n",
    ")\n",
    "df.loc[pd.isnull(df.Month), \"Month\"] = df.loc[pd.isnull(df.Month), \"Month\"].apply(\n",
    "    lambda x: random.choice(months)\n",
    ")\n",
    "df.loc[pd.isnull(df.Day), \"Day\"] = df.loc[\n",
    "    df.loc[pd.isnull(df.Day)].index, \"Month\"\n",
    "].apply(lambda x: random.randint(1, monthsMaxDays[x] + 1))\n",
    "df[\"Month\"] = df[\"Month\"].map(df[\"Month\"].value_counts())\n",
    "df[\"Day\"] = df[\"Day\"].map(df[\"Day\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff2a2f83-222e-4f0a-bf55-027c915d8dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Title', 'Author', 'Edition', 'Reviews', 'Ratings', 'Synopsis',\n",
       "       'Genre', 'BookCategory', 'Price', 'type', 'RatingValue', 'NumReview',\n",
       "       'EditionDate', 'Year', 'Month', 'Day', 'PrintEdition', 'IsImported',\n",
       "       'IsBook', 'IsMultipleAuthor', 'IsSpecialAuthor', 'IsFamousAuthor'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ff8e01-aab1-41bc-914a-8b0c2da142b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer] ........... (1 of 4) Processing cat, total=   0.0s\n",
      "[ColumnTransformer] .......... (2 of 4) Processing text, total=  28.2s\n",
      "[ColumnTransformer] .......... (3 of 4) Processing bool, total=   0.0s\n",
      "[ColumnTransformer] ........... (4 of 4) Processing int, total=   0.0s\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "categorical_features = [\"BookCategory\", \"Month\", \"PrintEdition\", \"Day\"]\n",
    "int_features = [\"Year\"]\n",
    "bool_features = [\n",
    "    \"IsBook\",\n",
    "    \"IsImported\",\n",
    "    \"IsMultipleAuthor\",\n",
    "    \"IsSpecialAuthor\",\n",
    "    \"IsFamousAuthor\",\n",
    "]\n",
    "text_features = [\"Synopsis\", \"Title\", \"Author\"]\n",
    "\n",
    "\n",
    "def lemmatize(s):\n",
    "    dff = []\n",
    "    for n, c in s.iteritems():\n",
    "        dff.append(\n",
    "            c.str.lower().apply(\n",
    "                lambda t: \" \".join(\n",
    "                    [Word(i).lemmatize() for i in t.split() if i not in stop]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    dff = pd.concat(dff, axis=1)\n",
    "    return dff\n",
    "\n",
    "\n",
    "def tovector(s):\n",
    "    dff = []\n",
    "    for n, c in s.iteritems():\n",
    "        dff.append(\n",
    "            c.apply(\n",
    "                lambda x: np.mean(\n",
    "                    [(word_model[i] if i in word_model else np.zeros(100)) for i in x],\n",
    "                    axis=0,\n",
    "                )\n",
    "                if x\n",
    "                else np.zeros(100)\n",
    "            )\n",
    "        )\n",
    "    dff = pd.concat(dff, axis=1)\n",
    "    return dff\n",
    "\n",
    "\n",
    "def text_vector_to_column(s):\n",
    "    dff = []\n",
    "    for n, c in s.iteritems():\n",
    "        for i in range(100):\n",
    "            dff.append(c.apply(lambda x: x[i]))\n",
    "    dff = pd.concat(dff, axis=1)\n",
    "    return dff\n",
    "\n",
    "\n",
    "def as_int(s):\n",
    "    dff = pd.concat([c.astype(int) for n, c in s.iteritems()], axis=1)\n",
    "    return dff\n",
    "\n",
    "\n",
    "def text_pro(s):\n",
    "    dff = []\n",
    "    for n, c in s.iteritems():\n",
    "        count_vectorizer = CountVectorizer()\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        count = count_vectorizer.fit_transform(c)\n",
    "        dff.append(pd.DataFrame.sparse.from_spmatrix(tfidf_vectorizer.fit_transform(c)))\n",
    "    return pd.concat(dff, axis=1)\n",
    "\n",
    "\n",
    "text_transformer = Pipeline(\n",
    "    [\n",
    "        (\"tokenize_lemmatize\", FunctionTransformer(lemmatize)),\n",
    "        (\"count_vectorizer\", FunctionTransformer(text_pro)),\n",
    "        #     ('tfidf', TfidfTransformer())\n",
    "        #     ('vector_to_column', FunctionTransformer(text_vector_to_column))\n",
    "        #     ('tfidf', TfidfTransformer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "bool_transformer = Pipeline([(\"to_int\", FunctionTransformer(as_int))])\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"onehot\", OneHotEncoder())\n",
    "        #     ('numeric', FunctionTransformer(lambda x: x, validate = False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "int_transformer = Pipeline([(\"minmaxscaler\", MinMaxScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "        (\"text\", text_transformer, text_features),\n",
    "        (\"bool\", bool_transformer, bool_features),\n",
    "        (\"int\", int_transformer, int_features),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "train_df = df[df.type == \"train\"]\n",
    "# train_df.head()\n",
    "y = np.array(train_df.Price).reshape(-1, 1)\n",
    "x = preprocessor.fit_transform(train_df)\n",
    "x\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# vectorizer.fit_transform(train_df['Synopsis'])\n",
    "# vectorizer.get_feature_names()\n",
    "# train_df.drop(['Price', 'type', 'index'], axis=1)\n",
    "# print(x.shape)\n",
    "# model = MLPRegressor(hidden_layer_sizes=(20, 20, 10), max_iter=1000)\n",
    "# model = RandomForestRegressor(n_estimators=1000)\n",
    "import math\n",
    "\n",
    "\n",
    "def metric_np(y_pred, y_true):\n",
    "    y_pred = math.e ** y_pred\n",
    "    y_true = math.e ** y_true\n",
    "    return 1 - np.sqrt(np.square(np.log10(y_pred + 1) - np.log10(y_true + 1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aaedde2-0ec4-464f-bf6a-020cc4f6d7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6237, 51403)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "539777bc-c17e-4b02-818e-9152e5387d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "sparse_matrix = scipy.sparse.csr_matrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a7db33f-55f8-428a-b2c1-8d2f9ffe4857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6237x51403 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 587967 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2b961f0d-26f2-4f97-a843-a2a4ff66244c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300+12+5+1+11+15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1416df2a-1feb-4645-94fb-19fd621dc9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def metric_np(y_pred, y_true):\n",
    "    y_pred = math.e**y_pred\n",
    "    y_true = math.e**y_true\n",
    "    return 1 - np.sqrt(np.square(np.log10(y_pred +1) - np.log10(y_true +1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e97e9a-15d3-472a-a308-720511dd1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "# model = MLPRegressor(hidden_layer_sizes=(20, 20, 10), max_iter=1000)\n",
    "model = LinearRegression()\n",
    "cross_validate(\n",
    "    model,\n",
    "    xtrain,\n",
    "    np.log(ytrain.ravel()),\n",
    "    scoring={\n",
    "        \"loss\": \"neg_mean_squared_error\",\n",
    "        \"metric\": make_scorer(metric_np, greater_is_better=True),\n",
    "    },\n",
    "    verbose=True,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "72f71049-1b90-4de6-9b5c-9896f8f10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-241-507526c7c540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m cross_validate(model, xtrain, np.log(ytrain), scoring={'loss': 'neg_mean_squared_error', 'metric': make_scorer(metric_tf, greater_is_better=True)}, \n\u001b[0;32m---> 36\u001b[0;31m                verbose=True, n_jobs=-1)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m# model.fit(xtrain, np.log(ytrain), callbacks=[PlotLossesKeras(), early_stopping], validation_data=(xtest, np.log(ytest)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 256\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;31m# For callabe scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def metric_tf(ypred, yval):\n",
    "    print('fuck')\n",
    "    ypred = math.e **ypred\n",
    "    yval = math.e**yval\n",
    "    return 1 - tf.math.reduce_mean(tf.sqrt(tf.square(log10(ypred + 1) - log10(yval + 1))))\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(40, input_dim=x.shape[-1], activation=\"relu\"))\n",
    "#     model.add(Dense(20, activation=\"relu\"))\n",
    "#     model.add(Dense(10, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer='adam',\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def log10(x):\n",
    "    numerator = tf.math.log(x)\n",
    "    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"mean_squared_error\",\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode=\"auto\"\n",
    ")\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n",
    "model = KerasRegressor(build_fn=create_model, epochs=1000, batch_size=10, verbose=1, callbacks=[early_stopping])\n",
    "cross_validate(model, xtrain, np.log(ytrain), scoring={'loss': 'neg_mean_squared_error', 'metric': make_scorer(metric_tf, greater_is_better=True)}, \n",
    "               verbose=True, n_jobs=-1)\n",
    "# model.fit(xtrain, np.log(ytrain), callbacks=[PlotLossesKeras(), early_stopping], validation_data=(xtest, np.log(ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64e13d-950b-4dbf-972a-d3cf547f655c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
