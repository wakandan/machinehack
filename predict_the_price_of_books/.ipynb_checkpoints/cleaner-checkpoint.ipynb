{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f8b0dfc0-bb51-420a-b525-775a3575a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c9f0977-8fd8-44c7-8602-fd26fbb29db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/anaconda3/envs/kaggle/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# prepare to generate word embbeding vectors\n",
    "glove_input_file = '/Users/kdang/Documents/glove.6B/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "word_model = KeyedVectors.load_word2vec_format(word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0f7b97d6-284c-4e2d-bc7d-86051032f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('Participants_Data/Data_Train.xlsx')\n",
    "test = pd.read_excel('Participants_Data/Data_Test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "563fc1a3-96ac-4e04-b022-9f78a31e442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['type'] = 'train'\n",
    "test['type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "fa4f3e7a-185c-4c6a-8008-b46847983b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3c9ff91d-7b70-4722-acde-cdb46f7c53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7797 entries, 0 to 7796\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   index         7797 non-null   int64  \n",
      " 1   Title         7797 non-null   object \n",
      " 2   Author        7797 non-null   object \n",
      " 3   Edition       7797 non-null   object \n",
      " 4   Reviews       7797 non-null   object \n",
      " 5   Ratings       7797 non-null   object \n",
      " 6   Synopsis      7797 non-null   object \n",
      " 7   Genre         7797 non-null   object \n",
      " 8   BookCategory  7797 non-null   object \n",
      " 9   Price         6237 non-null   float64\n",
      " 10  type          7797 non-null   object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 670.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ee718de2-092a-4bc8-a59e-8c171c3ac435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/anaconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/strings/accessor.py:101: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df['RatingValue'] = df['Reviews'].apply(lambda x: float(x.split()[0])/5)\n",
    "df['NumReview'] = df['Ratings'].apply(lambda x: int(x.split()[0].replace(',', '')))\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "monthsMaxDays = {\n",
    "    'Jan': 31, \n",
    "    'Feb': 28, \n",
    "    'Mar': 31, \n",
    "    'Apr': 30, \n",
    "    'May': 31, \n",
    "    'Jun': 30, \n",
    "    'Jul': 31, \n",
    "    'Aug': 31, \n",
    "    'Sep': 30, \n",
    "    'Oct': 31, \n",
    "    'Nov': 30, \n",
    "    'Dec': 31\n",
    "}\n",
    "months_to_num = {}\n",
    "for i, m in enumerate(months):\n",
    "    months_to_num[m] = i+1\n",
    "df['Edition'] = df.Edition.apply(lambda x: x.replace('â€“', '-'))\n",
    "df.loc[df.Edition.str.match('.*\\(\\w+\\).*'), 'Edition'] = df.loc[df.Edition.str.match('.*\\(\\w+\\).*'), 'Edition'].apply(lambda x: re.sub('\\(\\w+\\),', '', x))\n",
    "df.loc[df.Edition.str.match(r'.*[^\\d]+$'), 'EditionDate'] = np.nan # pattern 1, year and month \n",
    "df.loc[df.Edition.str.match(r'.*\\d+$'), 'EditionDate'] = df.Edition.apply(lambda x: x.split(',')[-1].split('-')[-1].strip())\n",
    "df['Year'] = df.EditionDate.apply(lambda x: int(x[-4:].strip()) if pd.notnull(x) else np.nan)\n",
    "df['Month'] = df.EditionDate.apply(lambda x: x[-8:-4] if (pd.notnull(x) and len(x)>8) else np.nan)\n",
    "df['Day'] = df.EditionDate.apply(lambda x: int(x[-11:-9].strip()) if (pd.notnull(x) and len(x)>=10) else np.nan)\n",
    "df['PrintEdition'] = df.Edition.apply(lambda x: x.split(',')[0])\n",
    "df['PrintEdition'] = df['PrintEdition'].map(df['PrintEdition'].value_counts()) \n",
    "df['IsImported'] = df.Edition.str.contains('Import')\n",
    "df['IsBook'] = df.Genre.str.contains('(Books)')\n",
    "df['IsMultipleAuthor'] = df.Author.str.match('.*[,&-].*')\n",
    "df['IsSpecialAuthor'] = df.Author.apply(lambda x: any(i for i in x.lower().split() if i in ['phd.', 'phd', 'dr', 'dr.', 'prof', 'prof.', 'sir', 'sir.', 'm.d.', 'm.d.', 'mr', 'mr.', 'mrs', 'mrs.', 'm.a', 'm.a.']))\n",
    "df['IsFamousAuthor'] = df.NumReview>100\n",
    "df['BookCategory'] = df['BookCategory'].map(df['BookCategory'].value_counts()) \n",
    "df['Genre'] = df['Genre'].map(df['Genre'].value_counts()) \n",
    "df.loc[pd.isnull(df.Year), 'Year'] = df.loc[pd.isnull(df.Year), 'Year'].apply(lambda x: random.randrange(df.Year.min(), df.Year.max()))\n",
    "df.loc[pd.isnull(df.Month), 'Month'] = df.loc[pd.isnull(df.Month), 'Month'].apply(lambda x: random.choice(months))\n",
    "df.loc[pd.isnull(df.Day), 'Day'] = df.loc[df.loc[pd.isnull(df.Day)].index, 'Month'].apply(lambda x: random.randint(1, monthsMaxDays[x]+1))\n",
    "df['Month'] = df['Month'].map(df['Month'].value_counts()) \n",
    "df['Day'] = df['Day'].map(df['Day'].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ff2a2f83-222e-4f0a-bf55-027c915d8dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Title', 'Author', 'Edition', 'Reviews', 'Ratings', 'Synopsis',\n",
       "       'Genre', 'BookCategory', 'Price', 'type', 'RatingValue', 'NumReview',\n",
       "       'EditionDate', 'Year', 'Month', 'Day', 'PrintEdition', 'IsImported',\n",
       "       'IsBook', 'IsMultipleAuthor', 'IsSpecialAuthor', 'IsFamousAuthor'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "32ff8e01-aab1-41bc-914a-8b0c2da142b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Title', 'Author', 'Edition', 'Reviews', 'Ratings', 'Synopsis',\n",
      "       'Genre', 'BookCategory', 'Price', 'type', 'RatingValue', 'NumReview',\n",
      "       'EditionDate', 'Year', 'Month', 'Day', 'PrintEdition', 'IsImported',\n",
      "       'IsBook', 'IsMultipleAuthor', 'IsSpecialAuthor', 'IsFamousAuthor'],\n",
      "      dtype='object')\n",
      "lemmatize returns 2\n",
      "tovector returns 2\n",
      "text vector to column returns 200\n",
      "as int returns 5\n",
      "(6237, 210)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    8.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([7.24502993, 6.60753512, 5.52481914, 6.97090316, 6.0361867 ]),\n",
       " 'score_time': array([0.00158191, 0.00151086, 0.00182199, 0.00159001, 0.00149417]),\n",
       " 'test_loss': array([-0.75254562, -0.67011413, -0.71006511, -0.85267786, -0.82025932]),\n",
       " 'test_metric': array([0.62448999, 0.64573818, 0.63531698, 0.60053671, 0.6079948 ])}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "categorical_features = ['BookCategory', 'Month', 'PrintEdition', 'Day']\n",
    "int_features = ['Year']\n",
    "bool_features = ['IsBook', 'IsImported', 'IsMultipleAuthor', 'IsSpecialAuthor', 'IsFamousAuthor']\n",
    "text_features = ['Synopsis', 'Title']\n",
    "\n",
    "def lemmatize(s):\n",
    "    dff = []\n",
    "    for n, c in s.iteritems():\n",
    "        dff.append(c.str.lower().apply(lambda t: [Word(i).lemmatize() for i in t.split() if i not in stop]))\n",
    "    dff = pd.concat(dff, axis=1)\n",
    "    print('lemmatize returns', len(dff.columns))\n",
    "    return dff\n",
    "\n",
    "def tovector(s):\n",
    "    dff = []\n",
    "    for n, c in s.iteritems():\n",
    "         dff.append(c.apply(\n",
    "            lambda x: np.mean(\n",
    "                [(word_model[i] if i in word_model else np.zeros(100)) for i in x], axis=0\n",
    "            )\n",
    "            if x\n",
    "            else np.zeros(100)\n",
    "        ))\n",
    "    dff = pd.concat(dff, axis=1)\n",
    "    print('tovector returns', len(dff.columns))\n",
    "    return dff\n",
    "\n",
    "def text_vector_to_column(s):\n",
    "    dff = []\n",
    "    for n, c in s.iteritems():\n",
    "        for i in range(100):\n",
    "            dff.append(c.apply(lambda x: x[i]))\n",
    "    dff = pd.concat(dff, axis=1)\n",
    "    print('text vector to column returns', len(dff.columns))\n",
    "    return dff\n",
    "\n",
    "def as_int(s):\n",
    "    dff = pd.concat([c.astype(int) for n, c in s.iteritems()], axis=1)\n",
    "    print('as int returns', len(dff.columns))\n",
    "    return dff\n",
    "    \n",
    "\n",
    "text_transformer = Pipeline([\n",
    "    ('tokenize_lemmatize', FunctionTransformer(lemmatize)),\n",
    "    ('count_vectorizer', FunctionTransformer(tovector)),\n",
    "    ('vector_to_column', FunctionTransformer(text_vector_to_column))\n",
    "])\n",
    "\n",
    "bool_transformer = Pipeline([\n",
    "    ('to_int', FunctionTransformer(as_int))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "#     ('onehot', OneHotEncoder())\n",
    "    ('numeric', FunctionTransformer(lambda x: x, validate = False))\n",
    "])\n",
    "\n",
    "int_transformer = Pipeline([\n",
    "    ('minmaxscaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('text', text_transformer, text_features),\n",
    "        ('bool', bool_transformer, bool_features),\n",
    "        ('int', int_transformer, int_features)\n",
    "    ])\n",
    "train_df = df[df.type=='train']\n",
    "# train_df.head()\n",
    "print(train_df.columns)\n",
    "y = np.array(train_df.Price).reshape(-1, 1)\n",
    "x = preprocessor.fit_transform(train_df.drop(['Price', 'type'], axis=1))\n",
    "print(x.shape)\n",
    "model = MLPRegressor(hidden_layer_sizes=(20, 20, 10), max_iter=1000)\n",
    "cross_validate(model, xtrain, np.log(ytrain.ravel()), scoring={'loss': 'neg_mean_squared_error', 'metric': make_scorer(metric_np, greater_is_better=True)}, \n",
    "               verbose=True,\n",
    "              n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4fd02ac9-2327-4649-addc-4217cb7a2bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6237, 356)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2b961f0d-26f2-4f97-a843-a2a4ff66244c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300+12+5+1+11+15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "576324da-7166-476c-8858-cfe8c104d92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 15)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.BookCategory.unique()), len(df.PrintEdition.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1416df2a-1feb-4645-94fb-19fd621dc9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def metric_np(y_pred, y_true):\n",
    "    y_pred = math.e**y_pred\n",
    "    y_true = math.e**y_true\n",
    "    return 1 - np.sqrt(np.square(np.log10(y_pred +1) - np.log10(y_true +1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d2e97e9a-15d3-472a-a308-720511dd1a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   10.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([ 6.8059082 , 10.28176904,  7.6437161 ,  9.17441106,  5.41213679]),\n",
       " 'score_time': array([0.00198197, 0.00158286, 0.00217605, 0.00180411, 0.00210023]),\n",
       " 'test_loss': array([-0.83457095, -0.83863212, -0.81899224, -0.73953159, -0.65524317]),\n",
       " 'test_metric': array([0.6046892 , 0.60361365, 0.6083765 , 0.62796102, 0.64978413])}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(20, 20, 10), max_iter=1000)\n",
    "cross_validate(model, xtrain, np.log(ytrain.ravel()), scoring={'loss': 'neg_mean_squared_error', 'metric': make_scorer(metric_np, greater_is_better=True)}, \n",
    "               verbose=True,\n",
    "              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f71049-1b90-4de6-9b5c-9896f8f10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "def metric_tf(ypred, yval):\n",
    "    print('fuck')\n",
    "    ypred = math.e **ypred\n",
    "    yval = math.e**yval\n",
    "    return 1 - tf.math.reduce_mean(tf.sqrt(tf.square(log10(ypred + 1) - log10(yval + 1))))\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(40, input_dim=x.shape[-1], activation=\"relu\"))\n",
    "#     model.add(Dense(20, activation=\"relu\"))\n",
    "#     model.add(Dense(10, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer='adam',\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def log10(x):\n",
    "    numerator = tf.math.log(x)\n",
    "    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"mean_squared_error\",\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode=\"auto\"\n",
    ")\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n",
    "model = KerasRegressor(build_fn=create_model, epochs=1000, batch_size=10, verbose=1, callbacks=[early_stopping])\n",
    "cross_validate(model, xtrain, np.log(ytrain), scoring={'loss': 'neg_mean_squared_error', 'metric': make_scorer(metric_tf, greater_is_better=True)}, \n",
    "               verbose=True, n_jobs=-1)\n",
    "# model.fit(xtrain, np.log(ytrain), callbacks=[PlotLossesKeras(), early_stopping], validation_data=(xtest, np.log(ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64e13d-950b-4dbf-972a-d3cf547f655c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
